# 深度学习模型评估中的几个“率”

在进行深度学习模型评估时，经常会遇到几个“率”的概念：准确率、精确率、召回率、F1值等。在详细介绍几个概念之前，我们先来了解几个名词：真阳性（True Positive）、假阳性（False Positive）、真阴性（True Negative）、假阴性（False Negative）。

假设在一个二分类问题场景下（如垃圾邮件分类，识别垃圾邮件定义为正类，非垃圾邮件定义为负类）：

1. 真阳性（True Positive）：
   - 定义：指模型**正确**的将**正样本**预测为**正类**的情况
   - 举例：将**垃圾邮件**预测为**垃圾邮件**
2. 假阳性（False Positive）：
   - 定义：指模型**错误**的将**正样本**预测为**负类**的情况
   - 举例：将**非垃圾邮件**预测为**垃圾邮件**
3. 真阴性（True Negative）：
   - 定义：指模型**正确**的将**负样本**预测为**负类**的情况
   - 举例：将**非垃圾邮件**预测为**非垃圾邮件**
4. 假阴性（False Negative）：
   - 定义：指模型**错误**的将**负样本**预测为**正类**的情况
   - 举例：将**垃圾邮件**预测为**非垃圾邮件**

有了上述几个概念之后，我们再来了解几个率：

## 1. 准确率（Accuracy）

准确率（Accuray）是深度学习模型中一种常用的评估指标，多用于衡量模型在分类任务中的预测效果，准确率表示模型预测正确的样本占所有样本的比例。其计算公式如下：

$$\text{Accuracy} = \frac{\text{True Positives (TP)} + \text{True Negatives (TN)}}{\text{Total Predictions}}$$

## 准确率的计算步骤

首先我们假定有一个分类问题，在此问题中，模型会输出预测的类别

1. 比较预测值与真实值：对于每个样本，将模型的预测值与真实值进行比较，判断其是否正确（包含 TP & TN 两类）
2. 统计正确预测的样本数：累加所有预测正确的样本数
3. 计算总样本数：统计用于评估的样本总数
4. 计算准确率：利用上面的公式，将正确预测的样本数除以总样本数，得到准确率

## 准确率的局限性

虽然准确率（Accuracy）是一个常见的评估指标，但在某些场景下，其仍然具有一些局限性，并不足以全面反映模型的性能，尤其是在以下场景中：

- 类别不平衡：当某一类别的数量远远多于其他类别时，准确率可能具有误导性。例如在一个包含95%正样本和5%负样本的分类问题中，如果模型总是预测所有样本为正类，那么其准确率将达到95%，但实际上模型并没有很好地学习到任何有用的信息（常用精确率、召回率、F1 score等其他评估指标代替）。
- 不适用于回归任务：准确率主要用于分类问题，而在回归任务中，需要使用均方误差（MSE）、平均绝对误差（MAE）等指标来评估模型的预测效果。
- 不适用于大语言模型：在生成式大语言模型（如GPT、BERT生成任务）中，准确率（Accuracy） 并不是最常用的评估指标。生成式模型主要处理的是自然语言生成任务，而这些任务的输出是连续的文本序列，因此准确率这种用于分类任务的评估方式不适合评估生成式模型的表现。
