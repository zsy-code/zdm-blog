# 深度学习模型评估中的几个“率”

在进行深度学习模型评估时，经常会遇到几个“率”的概念：准确率、精确率、召回率、F1值等。在详细介绍几个概念之前，我们先来了解几个名词：真阳性（True Positive）、假阳性（False Positive）、真阴性（True Negative）、假阴性（False Negative）。

假设在一个二分类问题场景下（如垃圾邮件分类，识别垃圾邮件定义为正类，非垃圾邮件定义为负类）：

1. 真阳性（True Positive）：
   - 定义：指模型**正确**的将**正样本**预测为**正类**的情况
   - 举例：将**垃圾邮件**预测为**垃圾邮件**
2. 假阳性（False Positive）：
   - 定义：指模型**错误**的将**正样本**预测为**负类**的情况
   - 举例：将**非垃圾邮件**预测为**垃圾邮件**
3. 真阴性（True Negative）：
   - 定义：指模型**正确**的将**负样本**预测为**负类**的情况
   - 举例：将**非垃圾邮件**预测为**非垃圾邮件**
4. 假阴性（False Negative）：
   - 定义：指模型**错误**的将**负样本**预测为**正类**的情况
   - 举例：将**垃圾邮件**预测为**非垃圾邮件**

有了上述几个概念之后，我们再来了解几个率：

## 1. 准确率（Accuracy）

准确率（Accuray）是深度学习模型中一种常用的评估指标，多用于衡量模型在分类任务中的预测效果，准确率表示模型**预测正确**的样本占**所有样本**的比例。其计算公式如下：

$$\text{Accuracy} = \frac{\text{True Positives (TP)} + \text{True Negatives (TN)}}{\text{Total Predictions}}$$

### 准确率的计算步骤

首先我们假定有一个分类问题，在此问题中，模型会输出预测的类别

1. 比较预测值与真实值：对于每个样本，将模型的预测值与真实值进行比较，判断其是否正确（包含 TP & TN 两类）
2. 统计正确预测的样本数：累加所有预测正确的样本数
3. 计算总样本数：统计用于评估的样本总数
4. 计算准确率：利用上面的公式，将正确预测的样本数除以总样本数，得到准确率

### 准确率的局限性

虽然准确率（Accuracy）是一个常见的评估指标，但在某些场景下，其仍然具有一些局限性，并不足以全面反映模型的性能，尤其是在以下场景中：

- 类别不平衡：当某一类别的数量远远多于其他类别时，准确率可能具有误导性。例如在一个包含95%正样本和5%负样本的分类问题中，如果模型总是预测所有样本为正类，那么其准确率将达到95%，但实际上模型并没有很好地学习到任何有用的信息（常用精确率、召回率、F1 score等其他评估指标代替）。
- 不适用于回归任务：准确率主要用于分类问题，而在回归任务中，需要使用均方误差（MSE）、平均绝对误差（MAE）等指标来评估模型的预测效果。
- 不适用于大语言模型：在生成式大语言模型（如GPT、BERT生成任务）中，准确率（Accuracy） 并不是最常用的评估指标。生成式模型主要处理的是自然语言生成任务，而这些任务的输出是连续的文本序列，因此准确率这种用于分类任务的评估方式不适合评估生成式模型的表现。

## 2. 精确率（Precision）

精确率（Precision）是深度学习模型中一种常用的评估指标，多用于衡量模型在分类任务中的准确程度。精确率表示模型预测为正类的样本中，真正为正类的比例。其计算公式如下：

$$\text{Precision} = \frac{\text{True Positives (TP)}}{\text{True Positives (TP)} + \text{False Positives (FP)}}$$

### 说明

- **精确率高**意味着模型在预测正类时很“谨慎”，多数正类预测都是正确的，但这并不意味着模型在找到所有正类（即召回率）方面表现良好
- 适合在**假阳性代价较高**的场景中使用，比如垃圾邮件过滤系统中，误将正常邮件识别为垃圾邮件的代价较高，因此要保证精确率高，减少不必要的误判

## 3. 召回率（Recall）

召回率（Recall）是衡量模型性能的另一个重要指标，多用于衡量模型在分类任务中的覆盖程度。召回率表示实际为**正类**的样本中，被模型**正确预测为正类**的比例。其计算公式如下：

$$\text{Recall} = \frac{\text{True Positives (TP)}}{\text{True Positives (TP)} + \text{False Negatives (FN)}}$$

### 说明

- **召回率高**意味着模型能够成功识别出大量正类样本，表示对正类的识别能力强
- 适合在**假阴性代价较高**的场景中使用，比如癌症检测系统中，漏诊比误诊更严重，因此要尽可能识别出所有癌症患者，因此召回率高很重要

## 4. F1分数（F1 Score）

F1分数（F1 Score）是精确率和召回率的加权平均值，其计算公式如下：

$$F_1 = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}$$

### 说明

- **精确率**衡量的是模型在预测为正类时的准确性，即预测为正类的样本中有多少是真正的正类。
- **召回率**衡量的是模型对实际正类样本的捕捉能力，即实际为正类的样本中有多少被正确预测为正类。

F1分数综合考虑了两者，既考虑了模型的准确性，又考虑了模型捕捉正类的能力。F1分数的范围在0到1之间，其中：

- **1**表示完美的精确率与召回率
- **0**表示精确率和召回率都非常差

### 优点

F1 分数在以下场景中非常有用：

- 类别不平衡：当正类和负类的数据比例非常不平衡时，单纯依赖准确率可能会具有误导性，而F1分数可以在类别不平衡的情况下更好的评估模型的性能。
- 精确率和召回率的权衡：在某些情况下，模型需要在高精确率和高召回率之间做出权衡，F1分数能够提供一个很好的折中方案，帮助评估模型在这两个指标下的综合表现。

### 局限性

- 平衡问题：F1分数会平衡精确率和召回率，但在某些应用中，可能精确率或召回率更为重要，例如，在一些场景中，漏掉一个正类样本（召回率低）可能比错判一个正类样本（精确率低）更严重，此时F1分数可能并不适用。
- 无法反映类别权重差异：F1分数假设正类和负类样本的重要性是一样的，但在某些实际任务中，不同类别的重要性可能不同。

### 为什么使用调和平均数而不是算术平均数

#### 1. 调和平均数的特性

公式：

$$H = \frac{2}{\frac{1}{x} + \frac{1}{y}} = \frac{2xy}{x+y}$$

- **调和平均数**是对两个数倒数的算术平均数的倒数。调和平均数在处理比率（如精确率和召回率）时特别有用，因为其对极端值（其中一个非常小）更加敏感。
- **调和平均数**强调平衡，会显著降低F1分数，如精确率或召回率中的任何一个非常低。这使得调和平均数在评估模型性能时更加严格，能够更好的反映模型在精确率和召回率之间的平衡。
- **调和平均数**对极端值更加敏感，能够更好地惩罚那些在精确率或召回率上表现不佳的模型。

#### 2. 算术平均数的局限性

- **算术平均数**是两个数的简单平均值。它对极端值不敏感，因此在精确率和召回率的情况下，算术平均数可能会掩盖其中的极端值。
- 例如，如果一个模型的精确率很高（0.9），但召回率很低（0.1），那么算术平均数（0.5），那么算术平均数给出的分数（0.5）就会掩盖低值，让我们误以为模型效果较好。

### 变体

- 微平均F1分数（Micro F1）：对每个类别的精确率和召回率按样本数量加权平均，适用于类别不平衡问题。
- 宏平均F1分数（Macro F1）：对每个类别的精确率和召回率取平均数，不考虑类别权重，适用于希望各个类别的表现都一样重要的场景。

****

> 除了特定算术公式计算的比率之外，还有一些使用图像和线下面积来进行评估的指标。在介绍此类指标之前，我们先来了解几个名词：
> 1. 真阳性率（Ture Positive Rate，TPR）：与召回率（Recall）相同，表示在所有实际为正类样本中，被正确预测为正类的比例。
>
>    计算公式如下：
>
>    $$\text{TPR} = \frac{\text{True Positives (TP)}}{\text{True Positives (TP)} + \text{False Negatives (FN)}}$$
>
> 2. 假阳性率（False Positive Rate，FPR）：表示在所有实际为负类样本中，被错误预测为正类的比例。
>
>    计算公式如下：
>
>    $$\text{FPR} = \frac{\text{False Positives (FP)}}{\text{False Positives (FP)} + \text{True Negatives (TN)}}$$

## 5. ROC 曲线下面积（AUC-ROC）

ROC 曲线是真阳性率（TPR）与假阳性率（FPR）的关系曲线，AUC-ROC 是该曲线下的面积。AUC-ROC（Area Under the Receiver Operating Characteristic Curve）最初是为二分类问题设计的，但它也可以扩展到多分类问题。

### 二分类问题中的AUC-ROC

- 公式（一般使用梯形法则近似）：

$$\text{AUC-ROC} = \int_0^1 \text{TPR}(\text{FPR}) d\text{FPR}$$

- 通用场景：在二分类问题中，AUC-ROC 用于评估模型在二分类任务中的性能，特别是在类别不平衡的情况下。

### 多分类问题中的AUC-ROC

#### 1）一对多（One-vs-Rest）

- 方法：对于每个类别，将其视为正类，其他所有类别视为负类，然后分别计算每个类别的ROC曲线和AUC-ROC。
- 优点：简单直观，易于理解
- 缺点：可能会忽略类别之间的相互关系

#### 2）宏平均（Macro-average）

- 方法：对每个类别分别计算ROC曲线和AUC-ROC，然后取所有类别的AUC-ROC平均值。
- 公式：

$$\text{MACRO-AUC-ROC} = \frac{1}{C} \sum_{i=1}^{C}\text{AUC-ROC}_i$$

- 优点：考虑了所有类别的贡献，适用于类别分布均衡的情况
- 缺点：对类别不平衡敏感，可能会掩盖少数类别的性能

#### 3）微平均（Micro-average）

- 方法：将所有类别的真阳性、假阳性、真阴性和假阴性样本合并，然后计算整体的ROC曲线和AUC-ROC。
- 公式：

$$\text{MICRO-AUC-ROC} = \text{AUC-ROC}(\text{TP}_{\text{total}}, \text{FP}_{\text{total}}, \text{TN}_{\text{total}}, \text{FN}_{\text{total}})$$

- 优点：适用于类别不平衡的情况，能够更好的反映整体性能
- 缺点：可能会忽略类别之间的差异

## 6. PR 曲线下面积（AUC-PR）

PR 曲线是精确率（Precision）与召回率（Recall）的关系曲线，AUC-PR 是该曲线下的面积。在类别不平衡的情况下，AUC-PR 比 AUC-ROC 更能反映模型的性能。

- 优点：特别适用于关注精确率和召回率的情况

### 计算方式

同ROC曲线下面积计算方式相同，PR曲线下面积计算也可以通过数值积分的方式来实现，但一般通过梯形法则来近似计算曲线下总面积。

- 公式（以梯形法则近似方法为例，假设有n个点 $(r_i, p_i)$，其中 $r_i$ 是召回率， $p_i$ 是精确率：

$$\text{AUC-PR} = \sum_{i=1}^{n-1} \frac{(r_{i+1} - r_{i}) \times (p_{i} + p_{i+1})}{2}$$

### 具体步骤

1. 获取PR曲线上的点：通过改变分类阈值，计算不同阈值下的精确率与召回率，得到一系列的点 $(r_i, p_i)$
2. 排序：将这些点按照召回率 $r_i$ 从小到大排序
3. 计算面积：使用梯形法则计算PR曲线下的面积